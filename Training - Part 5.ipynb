{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxBt9UZx/TBVor1JoAm6GP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahal-mulki/learning-pytorch/blob/main/Training%20-%20Part%205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkKxSloZzwOL",
        "outputId": "9ac98dcb-421c-44df-9748-68b685b7b5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 tensorboard-2.17.1 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade torch torchvision matplotlib tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "training_set, validation_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True), torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "training_loader, validation_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True), torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "print('Training set has {} instances'.format(len(training_set)))\n",
        "print('Validation set has {} instances'.format(len(validation_set)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gtfXw7Wzyzf",
        "outputId": "2ce08b91-efd4-4c2d-cfd6-d0009fdce4c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17483388.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 277225.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5040038.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 10844940.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Training set has 60000 instances\n",
            "Validation set has 10000 instances\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "print('  '.join(classes[labels[j]] for j in range(4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "HdC5KSfs1VBP",
        "outputId": "88591a8b-d6a1-4e88-c4ed-95592a4b1981"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-shirt/top  Bag  Ankle Boot  T-shirt/top\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class GarmentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GarmentClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = GarmentClassifier()"
      ],
      "metadata": {
        "id": "fA2Kk4Yl1zM3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
        "# Represents the model's confidence in each of the 10 classes for a given input\n",
        "dummy_outputs = torch.rand(4, 10)\n",
        "# Represents the correct class among the 10 being tested\n",
        "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "print('Total loss for this batch: {}'.format(loss.item()))"
      ],
      "metadata": {
        "id": "gszWQWUEhVh2",
        "outputId": "bc9897c3-7b2e-4d38-d73b-4eee57a91175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3888, 0.5167, 0.3887, 0.7157, 0.6999, 0.5098, 0.6968, 0.7181, 0.7863,\n",
            "         0.5913],\n",
            "        [0.6541, 0.6595, 0.5698, 0.9873, 0.9169, 0.6694, 0.8622, 0.5929, 0.8827,\n",
            "         0.2642],\n",
            "        [0.7491, 0.7023, 0.9722, 0.4955, 0.3817, 0.0958, 0.5693, 0.3204, 0.8394,\n",
            "         0.7396],\n",
            "        [0.5091, 0.1344, 0.4114, 0.9005, 0.1574, 0.2481, 0.1158, 0.4091, 0.2904,\n",
            "         0.2344]])\n",
            "tensor([1, 5, 3, 7])\n",
            "Total loss for this batch: 2.3603978157043457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "BqnXMxdihY5m"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    correct = 0\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            # Get the predicted class indices\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct += (predicted == labels).float().sum()\n",
        "\n",
        "            accuracy = 100 * correct / len(training_set)\n",
        "\n",
        "            print(\"Accuracy = {}\".format(accuracy))\n",
        "\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "Rbm-Yk8UhtBz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct"
      ],
      "metadata": {
        "id": "4CZjmsfYnBQl",
        "outputId": "b559a9ff-9c1c-49ed-a3d5-e4e8dbe127d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'correct' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-97d539c5f1c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'correct' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "dwiSLa1cjUqY",
        "outputId": "ad1aa87b-7fcc-4fd7-bcc0-b25529376d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "Accuracy = 0.0033333334140479565\n",
            "  batch 1000 loss: 0.6746898251026869\n",
            "Accuracy = 0.009999999776482582\n",
            "  batch 2000 loss: 0.6393106321841479\n",
            "Accuracy = 0.01666666753590107\n",
            "  batch 3000 loss: 0.6247236598413438\n",
            "Accuracy = 0.021666666492819786\n",
            "  batch 4000 loss: 0.62310949507216\n",
            "Accuracy = 0.028333334252238274\n",
            "  batch 5000 loss: 0.5974928291002288\n",
            "Accuracy = 0.03500000014901161\n",
            "  batch 6000 loss: 0.5837499859009404\n",
            "Accuracy = 0.03999999910593033\n",
            "  batch 7000 loss: 0.5749148722265381\n",
            "Accuracy = 0.046666666865348816\n",
            "  batch 8000 loss: 0.5416516042528674\n",
            "Accuracy = 0.05166666582226753\n",
            "  batch 9000 loss: 0.532583633831091\n",
            "Accuracy = 0.05833333358168602\n",
            "  batch 10000 loss: 0.541343234567903\n",
            "Accuracy = 0.06333333253860474\n",
            "  batch 11000 loss: 0.5193877103331033\n",
            "Accuracy = 0.06833333522081375\n",
            "  batch 12000 loss: 0.5094154637369793\n",
            "Accuracy = 0.07500000298023224\n",
            "  batch 13000 loss: 0.509224226657534\n",
            "Accuracy = 0.07999999821186066\n",
            "  batch 14000 loss: 0.48496226596692577\n",
            "Accuracy = 0.08666666597127914\n",
            "  batch 15000 loss: 0.4926216944793705\n",
            "LOSS train 0.4926216944793705 valid 0.5082350969314575\n",
            "EPOCH 2:\n",
            "Accuracy = 0.004999999888241291\n",
            "  batch 1000 loss: 0.4707017197459936\n",
            "Accuracy = 0.011666666716337204\n",
            "  batch 2000 loss: 0.48504376522847453\n",
            "Accuracy = 0.01666666753590107\n",
            "  batch 3000 loss: 0.44623406158864964\n",
            "Accuracy = 0.021666666492819786\n",
            "  batch 4000 loss: 0.45126330682914706\n",
            "Accuracy = 0.02666666731238365\n",
            "  batch 5000 loss: 0.44091910469916185\n",
            "Accuracy = 0.03333333507180214\n",
            "  batch 6000 loss: 0.43257176624960264\n",
            "Accuracy = 0.036666665226221085\n",
            "  batch 7000 loss: 0.4332772995538544\n",
            "Accuracy = 0.0416666679084301\n",
            "  batch 8000 loss: 0.4571110714527313\n",
            "Accuracy = 0.04833333194255829\n",
            "  batch 9000 loss: 0.455845424516825\n",
            "Accuracy = 0.0533333346247673\n",
            "  batch 10000 loss: 0.42454282346967376\n",
            "Accuracy = 0.05833333358168602\n",
            "  batch 11000 loss: 0.42933260681503455\n",
            "Accuracy = 0.06499999761581421\n",
            "  batch 12000 loss: 0.43241617497149853\n",
            "Accuracy = 0.0716666653752327\n",
            "  batch 13000 loss: 0.4253838768072892\n",
            "Accuracy = 0.07666666805744171\n",
            "  batch 14000 loss: 0.42714556370768697\n",
            "Accuracy = 0.07999999821186066\n",
            "  batch 15000 loss: 0.41535694732295814\n",
            "LOSS train 0.41535694732295814 valid 0.42211881279945374\n",
            "EPOCH 3:\n",
            "Accuracy = 0.004999999888241291\n",
            "  batch 1000 loss: 0.3916798390609911\n",
            "Accuracy = 0.009999999776482582\n",
            "  batch 2000 loss: 0.41445172115298917\n",
            "Accuracy = 0.01666666753590107\n",
            "  batch 3000 loss: 0.38920763870931113\n",
            "Accuracy = 0.023333333432674408\n",
            "  batch 4000 loss: 0.3886718154563569\n",
            "Accuracy = 0.028333334252238274\n",
            "  batch 5000 loss: 0.3835481841324945\n",
            "Accuracy = 0.03500000014901161\n",
            "  batch 6000 loss: 0.3845495872212341\n",
            "Accuracy = 0.0416666679084301\n",
            "  batch 7000 loss: 0.38117753515896036\n",
            "Accuracy = 0.04833333194255829\n",
            "  batch 8000 loss: 0.395336695908336\n",
            "Accuracy = 0.0533333346247673\n",
            "  batch 9000 loss: 0.3872203767988831\n",
            "Accuracy = 0.05999999865889549\n",
            "  batch 10000 loss: 0.3810960510144942\n",
            "Accuracy = 0.06666667014360428\n",
            "  batch 11000 loss: 0.38394685263046996\n",
            "Accuracy = 0.0716666653752327\n",
            "  batch 12000 loss: 0.3683388286513509\n",
            "Accuracy = 0.07666666805744171\n",
            "  batch 13000 loss: 0.39705747325671836\n",
            "Accuracy = 0.07999999821186066\n",
            "  batch 14000 loss: 0.36379995557159417\n",
            "Accuracy = 0.08666666597127914\n",
            "  batch 15000 loss: 0.3861785738804319\n",
            "LOSS train 0.3861785738804319 valid 0.3920324444770813\n",
            "EPOCH 4:\n",
            "Accuracy = 0.006666666828095913\n",
            "  batch 1000 loss: 0.36458126915094907\n",
            "Accuracy = 0.009999999776482582\n",
            "  batch 2000 loss: 0.35504712525114884\n",
            "Accuracy = 0.01666666753590107\n",
            "  batch 3000 loss: 0.3726227724659257\n",
            "Accuracy = 0.019999999552965164\n",
            "  batch 4000 loss: 0.3796930581375491\n",
            "Accuracy = 0.02666666731238365\n",
            "  batch 5000 loss: 0.3352954202434048\n",
            "Accuracy = 0.03333333507180214\n",
            "  batch 6000 loss: 0.3585428306319518\n",
            "Accuracy = 0.03999999910593033\n",
            "  batch 7000 loss: 0.3471448860094606\n",
            "Accuracy = 0.046666666865348816\n",
            "  batch 8000 loss: 0.3252559248527396\n",
            "Accuracy = 0.05166666582226753\n",
            "  batch 9000 loss: 0.3675025460656034\n",
            "Accuracy = 0.05666666850447655\n",
            "  batch 10000 loss: 0.3488444450990355\n",
            "Accuracy = 0.06333333253860474\n",
            "  batch 11000 loss: 0.3486360836396925\n",
            "Accuracy = 0.06833333522081375\n",
            "  batch 12000 loss: 0.3435220561788883\n",
            "Accuracy = 0.07500000298023224\n",
            "  batch 13000 loss: 0.3457944858481642\n",
            "Accuracy = 0.07666666805744171\n",
            "  batch 14000 loss: 0.33987812387656596\n",
            "Accuracy = 0.07999999821186066\n",
            "  batch 15000 loss: 0.3549321010544663\n",
            "LOSS train 0.3549321010544663 valid 0.3953488767147064\n",
            "EPOCH 5:\n",
            "Accuracy = 0.006666666828095913\n",
            "  batch 1000 loss: 0.3328030382876168\n",
            "Accuracy = 0.013333333656191826\n",
            "  batch 2000 loss: 0.33345244861702666\n",
            "Accuracy = 0.019999999552965164\n",
            "  batch 3000 loss: 0.3345712039103673\n",
            "Accuracy = 0.02666666731238365\n",
            "  batch 4000 loss: 0.3255991057950014\n",
            "Accuracy = 0.03333333507180214\n",
            "  batch 5000 loss: 0.3282427602381795\n",
            "Accuracy = 0.03999999910593033\n",
            "  batch 6000 loss: 0.3439444329174003\n",
            "Accuracy = 0.046666666865348816\n",
            "  batch 7000 loss: 0.3374554038568167\n",
            "Accuracy = 0.05166666582226753\n",
            "  batch 8000 loss: 0.3299129951390787\n",
            "Accuracy = 0.05833333358168602\n",
            "  batch 9000 loss: 0.319693095575436\n",
            "Accuracy = 0.06499999761581421\n",
            "  batch 10000 loss: 0.32194219248770967\n",
            "Accuracy = 0.07000000029802322\n",
            "  batch 11000 loss: 0.32716605674855237\n",
            "Accuracy = 0.07666666805744171\n",
            "  batch 12000 loss: 0.3280219143436407\n",
            "Accuracy = 0.07999999821186066\n",
            "  batch 13000 loss: 0.3270219654250686\n",
            "Accuracy = 0.0833333358168602\n",
            "  batch 14000 loss: 0.3127164442836074\n",
            "Accuracy = 0.08833333104848862\n",
            "  batch 15000 loss: 0.34204505727192736\n",
            "LOSS train 0.34204505727192736 valid 0.3458774983882904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZR7K1UkjcNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}